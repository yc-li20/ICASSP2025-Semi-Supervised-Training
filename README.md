Code for ICASSP 2025 submission

**Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling**

The bimodal fusion code is used from our SLT 2024 work:

[**Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques**](https://github.com/yc-li20/SER-on-WER-and-Fusion)

You may kindly cite

```
@article{li2024speech,
  title={Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques},
  author={Li, Yuanchao and Bell, Peter and Lai, Catherine},
  journal={arXiv preprint arXiv:2406.08353},
  year={2024}
}
```

The Frechet audio distance is implemented using [Microsoft FAD Toolkit](https://github.com/microsoft/fadtk)

The R3 prompt and parameter-efficient fine-tuning code is used from our another icassp 2025 submission:
[**Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction**](https://github.com/yc-li20/Emotion-Prompt)
