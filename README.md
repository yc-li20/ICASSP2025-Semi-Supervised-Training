Code for ICASSP 2025 submission

**Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling**

The bimodal fusion code is used from our SLT 2024 work:

[**Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques**](https://github.com/yc-li20/SER-on-WER-and-Fusion)

You may kindly cite

```
@inproceedings{li2024speech,
  title={Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques},
  author={Li, Yuanchao and Bell, Peter and Lai, Catherine},
  booktitle={2024 IEEE Spoken Language Technology Workshop (SLT)},
  pages={518--525},
  year={2024},
  organization={IEEE}
}
```

The Frechet audio distance is implemented using [Microsoft FAD Toolkit](https://github.com/microsoft/fadtk)

The R3 prompt and parameter-efficient fine-tuning code is used from our another icassp 2025 submission:
[**Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction**](https://github.com/yc-li20/Emotion-Prompt)
